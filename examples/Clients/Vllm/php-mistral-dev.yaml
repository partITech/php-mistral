services:
  vllm:
    image: vllm/vllm-openai:v0.7.3
    command: |
      --model PyrTools/Ministral-8B-Instruct-2410-AWQ
      --max_num_batched_tokens 1024
      --enable-auto-tool-choice
      --tool-call-parser mistral
      --served-model-name Ministral-8B-Instruct-2410
      --max-model-len 1024
      --tensor-parallel-size 1
      --gpu-memory-utilization 1
      --trust-remote-code
      --enforce-eager
      --api-key token-abc123
      --pipeline-parallel-size=1
    environment:
      - HUGGING_FACE_HUB_TOKEN=hf_****
      - MODEL
      - SERVED_MODEL_NAME
      - MAX_MODEL_LEN
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
      #- NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - /llm:/llm
      - /Data:/Data
      - /llm/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "40001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    runtime: nvidia
    ipc: host
  embedding:
    image: vllm/vllm-openai:v0.7.3
    command: |
      --model BAAI/bge-m3
      --enforce-eager
      --task embedding
    environment:
      - HUGGING_FACE_HUB_TOKEN=hf_****
      - MODEL
      - SERVED_MODEL_NAME
      - MAX_MODEL_LEN
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
      #- NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - /llm:/llm
      - /Data:/Data
      - /llm/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "40002:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    runtime: nvidia
    ipc: host

  transcription:
    build:
      context: .
      dockerfile: Dockerfile.vllm.phpMistral
    image: vllm-php-mistral:v0.7.3
    command: |
      --model openai/whisper-small
      --enforce-eager
      --task transcription
    environment:
      - HUGGING_FACE_HUB_TOKEN=hf_***
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
      #- NVIDIA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - /llm:/llm
      - /Data:/Data
      - /llm/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "40003:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    runtime: nvidia
    ipc: host
  rerank:
    image: vllm/vllm-openai:v0.7.3
    command: |
      --model BAAI/bge-reranker-v2-m3
      --served-model-name BAAI/bge-reranker-v2-m3
      --task score
    environment:
      - HUGGING_FACE_HUB_TOKEN=hf_***
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - /llm:/llm
      - /Data:/Data
      - /llm/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "40004:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    runtime: nvidia
    ipc: host